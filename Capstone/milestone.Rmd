---
title: "Capstone - Milestone report"
author: "martinvanelp"
date: "december 2015"
output: html_document
---

# Abstract
The data sets for the Capstone project have been analysed, mostly with help of the tm-package. The general conclusion is that with rather few words, or groups of words, in a dictionary one can cover large parts of the written language in these data sets. More specifically, about 200 words can cover 50% of the words used in tweets according to this analysis. Lastly, I am pondering whether I should continue on this path, with the tm-package, or make my own, more slim, approach to the Capstone challenge.

# Downloading and loading the data
The table beneath shows some statistics on the different data sets for the English language. In the code it is possible to easily replace "en_US" with another available language. Using the tm-package I have been able to filter for profanity, based on https://gist.github.com/ryanlewis/a37739d710ccdb4b406d. The tm-package does create bulky data with its functions, which means I have only been able to analyze samples of each data set (n = 10000).

The histogram after the table shows that there actually is data on every separate document in the data sets: the histogram shows the frequency of the length of tweets, being rather uniformly distributed.

```{r, echo=FALSE, message=FALSE, warning=FALSE}
# quick fix
setwd("C:/Users/Martin/Box Sync/Programming/GitHub/datasciencecoursera/Capstone")

# 1. Demonstrate that you've downloaded the data and have successfully loaded it in.
if (!file.exists("data")) { dir.create("data") }
if (!file.exists("data/Coursera-SwiftKey.zip")) {
        fileUrl <- "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
        download.file(fileUrl, destfile = "data/Coursera-SwiftKey.zip")
}
if (!file.exists("final")) { 
        unzip("data/Coursera-SwiftKey.zip")
}

library(dplyr)
library(tm)
library(caret)
library(SnowballC)
library(rJava)
library(RWeka)
library(knitr)

# constants
language <- "en_US"
lang_S   <- strsplit(language, split = "_")[[1]][1]

# Profanity word list
#      sources for profanity word list:
#      - https://gist.github.com/ryanlewis/a37739d710ccdb4b406d
con <- file("profanity.txt", open = "r")
profanity <- readLines(con)
close(con)

# load all documents for a language, workaround
con <- file(paste(getwd(), "/final/", language, 
                  "/", language, ".blogs.txt", sep = ""), open = "r")
blogs <- readLines(con)
Encoding(blogs) <- "UTF-8"
close(con)

con <- file(paste(getwd(), "/final/", language, 
                  "/", language, ".news.txt", sep = ""), open = "rb")
news <- readLines(con)
Encoding(news) <- "UTF-8"
close(con)

con <- file(paste(getwd(), "/final/", language, 
                  "/", language, ".twitter.txt", sep = ""), open = "r")
twitter <- readLines(con)
close(con)

# turn data into corpus
nL <- 10000

set.seed(1234)
blgVS   <- VectorSource(sample(blogs, nL))
blgDocs <- Corpus(blgVS, readerControl = list(reader = readPlain, 
                                              language = lang_S, 
                                              load = TRUE))
set.seed(1234)
nwsVS   <- VectorSource(sample(news, nL))
nwsDocs <- Corpus(nwsVS, readerControl = list(reader = readPlain, 
                                              language = lang_S, 
                                              load = TRUE))

set.seed(1234)
twtVS   <- VectorSource(sample(twitter, nL))
twtDocs <- Corpus(twtVS, readerControl = list(reader = readPlain, 
                                              language = lang_S, 
                                              load = TRUE))

# remove profanity, and for that convert to lower case
cleanup <- function(x) { x <- tm_map(x, content_transformer(tolower));
                         x <- tm_map(x, removeWords, profanity);
                         return(x) }

blgDocs <- cleanup(blgDocs)
nwsDocs <- cleanup(nwsDocs)
twtDocs <- cleanup(twtDocs)

# longest line
lline <- function(x) { Nchar <- lapply(x, nchar);
                       NcharM <- as.numeric(Nchar);
                       NcharM <- sort(NcharM, decreasing=TRUE);
                       return(NcharM)
}

docLength  <- rbind(length(blogs), length(news), length(twitter))
lblogs <- lline(blogs); lnews <- lline(news); ltwitter <- lline(twitter) 
docLongest <- rbind(head(lblogs,1), 
                    head(lnews,1),
                    head(ltwitter,1))
docAVGlong <- rbind(round(mean(lblogs),1), 
                    round(mean(lnews),1),
                    round(mean(ltwitter),1))

docStats <- data.frame(Lines = docLength, 
                       MeanLength = docAVGlong, 
                       LongestLine = docLongest,
                       row.names = c("blogs", "news", "twitter"))

kable(docStats)

hist(ltwitter)
```

# Statistics and findings
Beneath are figures and tables for each data set (blogs, news, and twitter) for unigrams, bigrams and trigrams. For the unigrams the data sets have been cleaned for stopwords (frequent words like "is", "the", "and").

The general conclusion based on the analyses of each data set is that rather few words are very common and that frequency of words quickly drops. Moreover, although there are differences many words are common in the blogs, news and twitter data sets. 

```{r, echo=FALSE, message=FALSE, warning=FALSE}
# 2. Create a basic report of summary statistics about the data sets.
# 3. Report any interesting findings that you amassed so far.

# tokenize functions
BiGramTokenizer <- function(x) 
        unlist(lapply(ngrams(words(x), 2), 
                      paste, collapse = " "), use.names = FALSE)
TriGramTokenizer <- function(x) 
        unlist(lapply(ngrams(words(x), 3), 
                      paste, collapse = " "), use.names = FALSE)

# explore
explore <- function(x) {
        # unigrams
        DTM  <- DocumentTermMatrix(x, 
                        control = list(removePunctuation = TRUE))
        sink("NUL")
        CS   <- colSums(inspect(removeSparseTerms(DTM, 0.995)))
        sink()
        CSw  <- sort(CS[!(names(CS) %in% stopwords("en"))], 
                     decreasing = TRUE)
        
        frequency <- head(CSw,25)
        plot(frequency,
        xlab = "Single words (excluding \"stopwords\")",
        ylab = "Frequency in 10k-line sample")
        text(1:length(frequency), 
             frequency, names(frequency), cex=0.6, pos=4, col="blue")
        print(as.data.frame(frequency))

        # bigrams
        DTMb <- DocumentTermMatrix(x, 
                                control = list(removePunctuation = TRUE,
                                               tokenize = BiGramTokenizer))
        sink("NUL")
        CSb  <- colSums(inspect(removeSparseTerms(DTMb, 0.999)))
        sink()
        
        frequency <- head(sort(CSb, decreasing = TRUE),25)
        plot(frequency,
             xlab = "Bigrams",
             ylab = "Frequency in 10k-line sample")
        text(1:length(frequency), 
             frequency, names(frequency), cex=0.6, pos=4, col="blue")
        print(as.data.frame(frequency))
        
        # trigrams
        DTMt <- DocumentTermMatrix(x, 
                          control = list(removePunctuation = TRUE,
                                         tokenize = TriGramTokenizer))
        sink("NUL")
        CSt  <- colSums(inspect(removeSparseTerms(DTMt, 0.999)))
        sink()        
        frequency <- head(sort(CSt, decreasing = TRUE),25)
        plot(frequency,
             xlab = "Trigrams",
             ylab = "Frequency in 10k-line sample")
        text(1:length(frequency), 
             frequency, names(frequency), cex=0.6, pos=4, col="blue")
        print(as.data.frame(frequency))
}
```

## Explore blogs
```{r, echo=FALSE, message=FALSE}
explore(blgDocs)
```

## Explore news
```{r, echo=FALSE, message=FALSE}
explore(nwsDocs)
```

## Explore twitter
```{r, echo=FALSE, message=FALSE}
explore(twtDocs)
```

## Explore desirable dictionary size
In the twitter data I also explored how many unique words are needed to have 50% of all word instances in "twitter" language. Given the table beneath this is a little over two hundred, based on the aforementioned sample.

```{r, echo=FALSE}
# 3. How many unique words do you need in a frequency sorted dictionary to cover 50% of all word instances in the language? 90%? 

DTMall <- DocumentTermMatrix(twtDocs,
                             control = list(removePunctuation = TRUE,
                                            removeNumbers = TRUE))
sink("NUL"); CSall <- colSums(inspect(DTMall)); sink()
CSall <- as.data.frame(sort(CSall, decreasing = TRUE))
names(CSall) <- "count"

CSall$cumulative <- 0
CSall$cumulative[1] <- CSall$count[1]
for (i in 2:length(CSall$count)) {
        CSall$cumulative[i] <- 
                CSall$cumulative[i-1] + CSall$count[i]
}
for (i in 1:length(CSall$count)) {
        CSall$cumShare[i] <-
                CSall$cumulative[i] / sum(CSall$count)
}
cbind(rank = 200:220,CSall[200:220,])
```

# Plan for prediction
```{r, echo=FALSE}
# 4. Get feedback on your plans for creating a prediction algorithm and Shiny app. 
```
Right now I am split between two options to build my prediction model:

1. build on top of the present analysis based in the tm-package, using the way that package tokenizes my data as predictors.
2. tokenize the data with a self-made method so I get more control of how tokens are created and what information is eventually available as predictors; I have some ideas on storing words as numbers and then using those numbers for a compact data set for prediction.

Clear from the exploratoy ranalysis is at least that in neither case many words/ngrams are needed to get some reasonable predictions. In favor of option 1 is that I already have some data cleaning and analysis set up. In favor of option 2 is that I can potentially create something without the tm-package that is less burdensome, and does not force me to use samples of data sets as above.